"""
Business Games - System Oceny KontraktÃ³w
ObsÅ‚uguje 3 tryby: Heurystyka, AI, Mistrz Gry
"""

import json
import random
from datetime import datetime
from typing import Dict, Tuple, Optional
import os

from config.business_games_settings import (
    EVALUATION_MODES,
    DEFAULT_EVALUATION_MODE,
    AI_EVALUATION_CONFIG,
    GAME_MASTER_CONFIG,
    HEURISTIC_CONFIG,
    SETTINGS_FILE
)

# =============================================================================
# GÅÃ“WNA FUNKCJA OCENY
# =============================================================================

def evaluate_contract_solution(
    user_data: Dict,
    contract: Dict,
    solution: str,
    evaluation_mode: Optional[str] = None
) -> Tuple[int, str, Dict]:
    """
    Ocenia rozwiÄ…zanie kontraktu wedÅ‚ug wybranego trybu
    
    Args:
        user_data: Dane uÅ¼ytkownika (potrzebne dla kolejki GM)
        contract: Dane kontraktu do oceny
        solution: Tekst rozwiÄ…zania przesÅ‚any przez uÅ¼ytkownika
        evaluation_mode: "heuristic" / "ai" / "game_master" (None = uÅ¼yj domyÅ›lnego)
    
    Returns:
        Tuple[rating, feedback, details]:
        - rating (int): Ocena 1-5 gwiazdek (0 = pending dla GM)
        - feedback (str): Tekstowy feedback dla uÅ¼ytkownika
        - details (dict): SzczegÃ³Å‚y oceny (metoda, statystyki itp.)
    
    Example:
        >>> rating, feedback, details = evaluate_contract_solution(
        ...     user_data, contract, solution, "heuristic"
        ... )
        >>> print(f"Ocena: {rating}/5")
        >>> print(f"Feedback: {feedback}")
    """
    # DEBUG: Loguj do pliku
    from utils.debug_log import log_debug
    log_debug("="*60)
    log_debug("evaluate_contract_solution() wywoÅ‚ane")
    
    # Pobierz aktywny tryb jeÅ›li nie podano
    if evaluation_mode is None:
        evaluation_mode = get_active_evaluation_mode()
        print(f"ðŸ” DEBUG: get_active_evaluation_mode() zwrÃ³ciÅ‚o: '{evaluation_mode}'")
        log_debug(f"get_active_evaluation_mode() zwrÃ³ciÅ‚o: '{evaluation_mode}'")
    
    print(f"ðŸ” DEBUG: UÅ¼ywam trybu oceny: '{evaluation_mode}'")
    log_debug(f"UÅ¼ywam trybu oceny: '{evaluation_mode}'")
    
    # Walidacja trybu
    if evaluation_mode not in EVALUATION_MODES:
        print(f"âš ï¸ Nieznany tryb oceny: {evaluation_mode}, fallback do heurystyki")
        log_debug(f"âš ï¸ Nieznany tryb oceny: {evaluation_mode}, fallback do heurystyki")
        evaluation_mode = "heuristic"
    
    # WybÃ³r metody oceny
    if evaluation_mode == "heuristic":
        return evaluate_heuristic(solution, contract)
    
    elif evaluation_mode == "ai":
        return evaluate_with_ai(solution, contract)
    
    elif evaluation_mode == "game_master":
        return queue_for_game_master(user_data, contract, solution)
    
    else:
        # Fallback do heurystyki
        return evaluate_heuristic(solution, contract)


# =============================================================================
# TRYB 1: HEURYSTYKA (prosta automatyczna ocena)
# =============================================================================

def evaluate_heuristic(solution: str, contract: Dict) -> Tuple[int, str, Dict]:
    """
    Prosta ocena oparta na dÅ‚ugoÅ›ci tekstu i losowoÅ›ci
    
    Algorytm:
    1. Policz sÅ‚owa w rozwiÄ…zaniu
    2. PorÃ³wnaj z minimum wymaganym dla kontraktu
    3. Przypisz bazowÄ… ocenÄ™ wedÅ‚ug progÃ³w
    4. Dodaj losowoÅ›Ä‡ Â±1 gwiazdka
    5. ZwrÃ³Ä‡ ocenÄ™ 1-5
    
    Args:
        solution: Tekst rozwiÄ…zania
        contract: Dane kontraktu (zawiera min_slow)
    
    Returns:
        (rating, feedback, details)
    """
    word_count = len(solution.split())
    min_words = contract.get("min_slow", 300)
    
    # Progi z konfiguracji
    thresholds = HEURISTIC_CONFIG["word_thresholds"]
    
    # Bazowa ocena na podstawie dÅ‚ugoÅ›ci
    if word_count < min_words * thresholds["min_multiplier"]:
        base_score = 1
    elif word_count < min_words * thresholds["low_multiplier"]:
        base_score = 2
    elif word_count < min_words * thresholds["med_multiplier"]:
        base_score = 3
    elif word_count < min_words * thresholds["high_multiplier"]:
        base_score = 4
    else:
        base_score = 5
    
    # LosowoÅ›Ä‡ (jeÅ›li wÅ‚Ä…czona)
    if HEURISTIC_CONFIG["randomness_enabled"]:
        randomness_range = HEURISTIC_CONFIG["randomness_range"]
        randomness = random.randint(*randomness_range)
        rating = max(1, min(5, base_score + randomness))
    else:
        rating = base_score
    
    # Feedback
    if HEURISTIC_CONFIG["auto_feedback"]:
        feedback = HEURISTIC_CONFIG["feedback_template"].format(
            word_count=word_count,
            rating=rating
        )
    else:
        feedback = f"Ocena: {rating}/5 â­"
    
    # SzczegÃ³Å‚y
    details = {
        "method": "heuristic",
        "word_count": word_count,
        "min_required": min_words,
        "base_score": base_score,
        "randomness": rating - base_score if HEURISTIC_CONFIG["randomness_enabled"] else 0,
        "final_rating": rating,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    return rating, feedback, details


# =============================================================================
# TRYB 2: OCENA AI (Google Gemini)
# =============================================================================

def evaluate_with_ai(solution: str, contract: Dict) -> Tuple[int, str, Dict]:
    """
    Ocena przez model Google Gemini
    
    WysyÅ‚a prompt do Gemini z:
    - Opisem kontraktu
    - RozwiÄ…zaniem uczestnika
    - Kryteriami oceny
    
    AI zwraca:
    - OcenÄ™ 0-100 punktÃ³w
    - Rating 1-5 gwiazdek
    - Feedback tekstowy
    - Mocne strony
    - Sugestie do poprawy
    
    Args:
        solution: Tekst rozwiÄ…zania
        contract: Dane kontraktu
    
    Returns:
        (rating, feedback, details)
    """
    from utils.debug_log import log_debug
    
    log_debug("ðŸ¤– evaluate_with_ai() ROZPOCZÄ˜TA")
    print("ðŸ¤– DEBUG: evaluate_with_ai() rozpoczÄ™ta")
    
    try:
        log_debug("ImportujÄ™ google.generativeai...")
        import google.generativeai as genai
        log_debug("âœ… Import google.generativeai OK")
        
        log_debug("PrzygotowujÄ™ prompt...")
        # Przygotuj prompt
        prompt = AI_EVALUATION_CONFIG["prompt_template"].format(
            contract_title=contract["tytul"],
            contract_category=contract["kategoria"],
            contract_difficulty=contract.get("trudnosc", 3),  # Poprawiono z "poziom_trudnosci" na "trudnosc"
            contract_description=contract["opis"],
            min_words=contract.get("min_slow", 300),
            user_solution=solution
        )
        log_debug(f"âœ… Prompt przygotowany (dÅ‚ugoÅ›Ä‡: {len(prompt)} znakÃ³w)")
        
        log_debug("Pobieram API key...")
        # Pobierz API key (priorytet: secrets > env > plik)
        api_key = None
        
        # 1. Najpierw sprawdÅº Streamlit secrets (standardowy sposÃ³b w aplikacji)
        try:
            import streamlit as st
            api_key = st.secrets.get("GOOGLE_API_KEY")
            if api_key:
                log_debug("âœ… API key pobrane z st.secrets")
        except Exception as e:
            log_debug(f"âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ z st.secrets: {e}")
        
        # 2. JeÅ›li nie ma w secrets, sprawdÅº zmiennÄ… Å›rodowiskowÄ…
        if not api_key:
            api_key = os.getenv("GOOGLE_API_KEY")
            if api_key:
                log_debug("âœ… API key pobrane z ENV")
        
        # 3. JeÅ›li nie ma, sprawdÅº plik konfiguracyjny (backward compatibility)
        if not api_key:
            api_key = load_gemini_api_key()
            if api_key:
                log_debug("âœ… API key pobrane z pliku")
        
        if not api_key:
            log_debug("âŒ BRAK API KEY! Rzucam wyjÄ…tek...")
            raise Exception("Brak klucza API Google Gemini. Fallback do heurystyki.")
        
        log_debug("KonfigurujÄ™ genai.configure()...")
        # Skonfiguruj Gemini
        genai.configure(api_key=api_key)
        log_debug("âœ… genai.configure() OK")
        
        log_debug("Ustawiam safety_settings...")
        # Safety settings - wyÅ‚Ä…cz zbyt restrykcyjne blokady dla treÅ›ci edukacyjnych
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        log_debug("âœ… safety_settings ustawione")
        
        log_debug(f"TworzÄ™ model: {AI_EVALUATION_CONFIG['model']}...")
        # UtwÃ³rz model
        model = genai.GenerativeModel(
            model_name=AI_EVALUATION_CONFIG["model"],
            generation_config=AI_EVALUATION_CONFIG["generation_config"],
            system_instruction=AI_EVALUATION_CONFIG["system_instruction"],
            safety_settings=safety_settings
        )
        log_debug("âœ… Model utworzony")
        
        log_debug("WywoÅ‚ujÄ™ model.generate_content()...")
        # WywoÅ‚aj Gemini API
        response = model.generate_content(prompt)
        log_debug("âœ… Otrzymano odpowiedÅº z Gemini")
        
        # SprawdÅº czy odpowiedÅº jest zablokowana przez safety
        if not response.candidates or not response.candidates[0].content.parts:
            # OdpowiedÅº zablokowana - sprÃ³buj bez safety settings
            print("âš ï¸ OdpowiedÅº zablokowana przez safety settings, prÃ³bujÄ™ ponownie bez nich...")
            model_no_safety = genai.GenerativeModel(
                model_name=AI_EVALUATION_CONFIG["model"],
                generation_config=AI_EVALUATION_CONFIG["generation_config"],
                system_instruction=AI_EVALUATION_CONFIG["system_instruction"]
            )
            response = model_no_safety.generate_content(prompt)
        
        # Parse odpowiedzi JSON
        result_text = response.text.strip()
        log_debug(f"DÅ‚ugoÅ›Ä‡ odpowiedzi: {len(result_text)} znakÃ³w")
        
        # UsuÅ„ markdown formatting jeÅ›li jest
        if result_text.startswith("```json"):
            result_text = result_text.replace("```json", "").replace("```", "").strip()
        elif result_text.startswith("```"):
            result_text = result_text.replace("```", "").strip()
        
        # SprÃ³buj naprawiÄ‡ uciÄ™ty JSON (dodaj brakujÄ…ce zamkniÄ™cia)
        if not result_text.endswith("}"):
            log_debug("âš ï¸ JSON nie koÅ„czy siÄ™ }, prÃ³bujÄ™ naprawiÄ‡...")
            # Policz otwarte/zamkniÄ™te nawiasy
            open_braces = result_text.count("{")
            close_braces = result_text.count("}")
            open_brackets = result_text.count("[")
            close_brackets = result_text.count("]")
            
            # Dodaj brakujÄ…ce zamkniÄ™cia
            if open_brackets > close_brackets:
                result_text += "]" * (open_brackets - close_brackets)
            if open_braces > close_braces:
                result_text += "}" * (open_braces - close_braces)
            
            log_debug(f"Naprawiony JSON: {result_text[-100:]}")
        
        result = json.loads(result_text)
        log_debug("âœ… JSON sparsowany pomyÅ›lnie")
        
        # WyciÄ…gnij dane
        rating = result.get("rating", 3)
        total_score = result.get("total_score", 50)
        feedback_text = result.get("feedback", "Brak feedbacku")
        strengths = result.get("strengths", [])
        improvements = result.get("improvements", [])
        
        # Sformatuj feedback (bez redundantnego nagÅ‚Ã³wka - ocena jest juÅ¼ widoczna w UI)
        feedback = feedback_text
        
        if strengths:
            feedback += "\n\n**âœ… Mocne strony:**\n"
            feedback += "\n".join([f"- {s}" for s in strengths])
        
        if improvements:
            feedback += "\n\n**ðŸ’¡ Do poprawy:**\n"
            feedback += "\n".join([f"- {i}" for i in improvements])
        
        # SzczegÃ³Å‚y
        details = {
            "method": "ai",
            "model": AI_EVALUATION_CONFIG["model"],
            "total_score": total_score,
            "rating": rating,
            "strengths": strengths,
            "improvements": improvements,
            "word_count": len(solution.split()),
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "provider": "google_gemini"
        }
        
        return rating, feedback, details
        
    except ImportError as e:
        log_debug(f"âŒ ImportError: {e}")
        print("âš ï¸ Brak biblioteki google-generativeai. Instaluj: pip install google-generativeai")
        log_debug("Fallback do heurystyki (ImportError)")
        return evaluate_heuristic(solution, contract)
    
    except json.JSONDecodeError as e:
        log_debug(f"âŒ JSONDecodeError: {e}")
        print(f"âš ï¸ BÅ‚Ä…d parsowania JSON z Gemini: {e}")
        try:
            print(f"OdpowiedÅº: {result_text[:200]}")
            log_debug(f"OdpowiedÅº Gemini: {result_text[:500]}")
        except:
            print("OdpowiedÅº: brak")
            log_debug("OdpowiedÅº Gemini: brak")
        print("Fallback do heurystyki...")
        log_debug("Fallback do heurystyki (JSONDecodeError)")
        return evaluate_heuristic(solution, contract)
    
    except Exception as e:
        # Fallback do heurystyki w razie bÅ‚Ä™du
        log_debug(f"âŒ Exception: {type(e).__name__}: {e}")
        print(f"âš ï¸ BÅ‚Ä…d oceny AI: {e}")
        print("Fallback do heurystyki...")
        log_debug("Fallback do heurystyki (Exception)")
        return evaluate_heuristic(solution, contract)


# =============================================================================
# TRYB 3: MISTRZ GRY (kolejka oczekujÄ…cych)
# =============================================================================

def queue_for_game_master(
    user_data: Dict,
    contract: Dict,
    solution: str
) -> Tuple[int, str, Dict]:
    """
    Dodaje rozwiÄ…zanie do kolejki oczekujÄ…cych na ocenÄ™ Mistrza Gry
    
    RozwiÄ…zanie nie jest od razu oceniane - trafia do kolejki.
    Admin zaloguje siÄ™ pÃ³Åºniej i przejrzy wszystkie oczekujÄ…ce.
    Po ocenie przez Admina kontrakt zostanie sfinalizowany.
    
    Args:
        user_data: Dane uÅ¼ytkownika (username, degencoins itp.)
        contract: Dane kontraktu
        solution: Tekst rozwiÄ…zania
    
    Returns:
        (0, feedback, details) - rating=0 oznacza "pending"
    """
    username = user_data.get("username", "unknown")
    review_id = f"review_{username}_{contract['id']}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    
    # Przygotuj rekord do kolejki
    pending_review = {
        "id": review_id,
        "username": username,
        "user_display_name": user_data.get("name", username),
        "contract_id": contract["id"],
        "contract_title": contract["tytul"],
        "contract_category": contract["kategoria"],
        "contract_difficulty": contract["poziom_trudnosci"],
        "contract_description": contract["opis"],
        "contract_min_words": contract.get("min_slow", 300),
        "solution": solution,
        "word_count": len(solution.split()),
        "submitted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "status": "pending",
        "rating": None,
        "feedback": None,
        "reviewed_by": None,
        "reviewed_at": None
    }
    
    # ZaÅ‚aduj kolejkÄ™
    queue = load_game_master_queue()
    
    # SprawdÅº limit
    max_pending = GAME_MASTER_CONFIG["max_pending_reviews"]
    pending_count = len([r for r in queue if r["status"] == "pending"])
    
    if pending_count >= max_pending:
        # Kolejka peÅ‚na - fallback do heurystyki
        print(f"âš ï¸ Kolejka Mistrza Gry peÅ‚na ({pending_count}/{max_pending}). Fallback do heurystyki.")
        return evaluate_heuristic(solution, contract)
    
    # Dodaj do kolejki
    queue.append(pending_review)
    
    # Zapisz
    save_game_master_queue(queue)
    
    # Feedback dla uÅ¼ytkownika
    feedback = f"""âœ… **RozwiÄ…zanie przesÅ‚ane!**

Twoje rozwiÄ…zanie oczekuje na ocenÄ™ przez Mistrza Gry.
Otrzymasz powiadomienie gdy zostanie ocenione.

â±ï¸ Szacowany czas oceny: {GAME_MASTER_CONFIG['review_deadline_hours']}h
ðŸ“Š Liczba sÅ‚Ã³w: {pending_review['word_count']}
"""
    
    # SzczegÃ³Å‚y
    details = {
        "method": "game_master",
        "review_id": review_id,
        "status": "pending",
        "word_count": pending_review['word_count'],
        "queue_position": pending_count + 1,
        "timestamp": pending_review["submitted_at"]
    }
    
    return 0, feedback, details  # rating=0 oznacza "pending"


def submit_game_master_review(
    review_id: str,
    rating: int,
    feedback: str,
    admin_username: str
) -> bool:
    """
    Admin zatwierdza ocenÄ™ rozwiÄ…zania
    
    WywoÅ‚ywane z panelu admina gdy Mistrz Gry oceni rozwiÄ…zanie.
    Aktualizuje status w kolejce i finalizuje kontrakt uÅ¼ytkownika.
    
    Args:
        review_id: ID rekordu z kolejki
        rating: Ocena 1-5 gwiazdek
        feedback: Komentarz od Admina
        admin_username: Username Mistrza Gry
    
    Returns:
        True jeÅ›li sukces, False jeÅ›li bÅ‚Ä…d
    """
    queue = load_game_master_queue()
    
    # ZnajdÅº review
    for review in queue:
        if review["id"] == review_id and review["status"] == "pending":
            # Zaktualizuj status
            review["status"] = "reviewed"
            review["rating"] = rating
            review["feedback"] = feedback
            review["reviewed_by"] = admin_username
            review["reviewed_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # Zapisz kolejkÄ™
            save_game_master_queue(queue)
            
            # Finalizuj kontrakt uÅ¼ytkownika
            success = finalize_game_master_contract(review)
            
            return success
    
    return False


def finalize_game_master_contract(review: Dict) -> bool:
    """
    Finalizuje kontrakt po ocenie przez Mistrza Gry
    
    Dodaje monety i reputacjÄ™ uÅ¼ytkownikowi, przenosi kontrakt do completed.
    
    Args:
        review: Rekord z kolejki z ocenÄ…
    
    Returns:
        True jeÅ›li sukces
    """
    try:
        from data.users import load_user_data, save_user_data
        from utils.business_game import calculate_contract_reward
        
        users_data = load_user_data()
        username = review["username"]
        
        if username not in users_data:
            print(f"âš ï¸ Nie znaleziono uÅ¼ytkownika: {username}")
            return False
        
        user_data = users_data[username]
        business_data = user_data.get("business_game", {})
        
        if not business_data:
            print(f"âš ï¸ UÅ¼ytkownik {username} nie ma Business Games")
            return False
        
        # ZnajdÅº aktywny kontrakt
        active_contracts = business_data.get("contracts", {}).get("active", [])
        contract_found = None
        
        for contract in active_contracts:
            if contract["id"] == review["contract_id"]:
                contract_found = contract
                break
        
        if not contract_found:
            print(f"âš ï¸ Nie znaleziono aktywnego kontraktu {review['contract_id']}")
            return False
        
        # Oblicz nagrodÄ™
        reward = calculate_contract_reward(contract_found, review["rating"], business_data)
        
        # Dodaj monety i reputacjÄ™
        user_data['degencoins'] = user_data.get('degencoins', 0) + reward["coins"]
        business_data["firm"]["reputation"] += reward["reputation"]
        business_data["stats"]["total_revenue"] += reward["coins"]
        
        # Zaktualizuj statystyki ocen
        rating_key = f"contracts_{review['rating']}star"
        business_data["stats"][rating_key] = business_data["stats"].get(rating_key, 0) + 1
        business_data["stats"]["contracts_completed"] = business_data["stats"].get("contracts_completed", 0) + 1
        
        # PrzenieÅ› do completed
        completed_contract = contract_found.copy()
        completed_contract["solution"] = review["solution"]
        completed_contract["rating"] = review["rating"]
        completed_contract["feedback"] = review["feedback"]
        completed_contract["reward"] = reward
        completed_contract["completed_date"] = review["reviewed_at"]
        completed_contract["evaluated_by"] = review["reviewed_by"]
        completed_contract["evaluation_method"] = "game_master"
        
        business_data["contracts"]["completed"].append(completed_contract)
        business_data["contracts"]["active"].remove(contract_found)
        
        # Zapisz
        user_data["business_game"] = business_data
        users_data[username] = user_data
        save_user_data(users_data)
        
        print(f"âœ… Kontrakt sfinalizowany dla {username}: +{reward['coins']} monet, +{reward['reputation']} reputacji")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d finalizacji kontraktu: {e}")
        import traceback
        traceback.print_exc()
        return False


# =============================================================================
# FUNKCJE POMOCNICZE - KONFIGURACJA
# =============================================================================

def get_active_evaluation_mode() -> str:
    """Pobiera aktualny tryb oceny z pliku konfiguracyjnego"""
    from utils.debug_log import log_debug
    
    print(f"ðŸ” DEBUG: get_active_evaluation_mode() wywoÅ‚a siÄ™...")
    print(f"ðŸ” DEBUG: SETTINGS_FILE = {SETTINGS_FILE}")
    print(f"ðŸ” DEBUG: DEFAULT_EVALUATION_MODE = {DEFAULT_EVALUATION_MODE}")
    
    log_debug("get_active_evaluation_mode() wywoÅ‚ane")
    log_debug(f"SETTINGS_FILE = {SETTINGS_FILE}")
    log_debug(f"DEFAULT_EVALUATION_MODE = {DEFAULT_EVALUATION_MODE}")
    
    try:
        if os.path.exists(SETTINGS_FILE):
            print(f"ðŸ” DEBUG: Plik {SETTINGS_FILE} istnieje")
            log_debug(f"Plik {SETTINGS_FILE} istnieje")
            with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                mode = config.get("evaluation_mode", DEFAULT_EVALUATION_MODE)
                print(f"ðŸ” DEBUG: Z pliku odczytano tryb: '{mode}'")
                log_debug(f"Z pliku odczytano tryb: '{mode}'")
                return mode
        else:
            print(f"ðŸ” DEBUG: Plik {SETTINGS_FILE} NIE istnieje, uÅ¼yjÄ™ domyÅ›lnego")
            log_debug(f"Plik {SETTINGS_FILE} NIE istnieje")
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d wczytywania trybu oceny: {e}")
        log_debug(f"âš ï¸ BÅ‚Ä…d: {e}")
    
    print(f"ðŸ” DEBUG: Zwracam DEFAULT: '{DEFAULT_EVALUATION_MODE}'")
    log_debug(f"Zwracam DEFAULT: '{DEFAULT_EVALUATION_MODE}'")
    return DEFAULT_EVALUATION_MODE


def set_active_evaluation_mode(mode: str) -> bool:
    """Ustawia aktywny tryb oceny"""
    try:
        if mode not in EVALUATION_MODES:
            print(f"âš ï¸ Nieznany tryb: {mode}")
            return False
        
        config = {"evaluation_mode": mode, "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        
        os.makedirs(os.path.dirname(SETTINGS_FILE), exist_ok=True)
        
        with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Tryb oceny zmieniony na: {EVALUATION_MODES[mode]['name']}")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d zapisywania trybu: {e}")
        return False


def load_gemini_api_key() -> Optional[str]:
    """Åaduje klucz API Google Gemini z pliku konfiguracyjnego"""
    try:
        api_key_file = "config/gemini_api_key.txt"
        if os.path.exists(api_key_file):
            with open(api_key_file, 'r') as f:
                return f.read().strip()
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d wczytywania API key: {e}")
    
    return None


def save_gemini_api_key(api_key: str) -> bool:
    """Zapisuje klucz API Google Gemini do pliku"""
    try:
        api_key_file = "config/gemini_api_key.txt"
        os.makedirs(os.path.dirname(api_key_file), exist_ok=True)
        
        with open(api_key_file, 'w') as f:
            f.write(api_key.strip())
        
        print("âœ… Klucz API Gemini zapisany")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d zapisywania API key: {e}")
        return False


# =============================================================================
# FUNKCJE POMOCNICZE - KOLEJKA MISTRZA GRY
# =============================================================================

def load_game_master_queue() -> list:
    """Åaduje kolejkÄ™ oczekujÄ…cych rozwiÄ…zaÅ„"""
    queue_file = GAME_MASTER_CONFIG["queue_file"]
    
    try:
        if os.path.exists(queue_file):
            with open(queue_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d wczytywania kolejki: {e}")
    
    return []


def save_game_master_queue(queue: list) -> bool:
    """Zapisuje kolejkÄ™"""
    queue_file = GAME_MASTER_CONFIG["queue_file"]
    
    try:
        with open(queue_file, 'w', encoding='utf-8') as f:
            json.dump(queue, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d zapisywania kolejki: {e}")
        return False


def get_pending_reviews_count() -> int:
    """Liczba oczekujÄ…cych ocen"""
    queue = load_game_master_queue()
    return len([r for r in queue if r["status"] == "pending"])


def get_pending_contract_reviews() -> list:
    """
    Pobiera listÄ™ oczekujÄ…cych rozwiÄ…zaÅ„ z dodatkowymi informacjami
    
    Returns:
        Lista recenzji z dodanym polem 'waiting_hours'
    """
    queue = load_game_master_queue()
    pending = [r for r in queue if r["status"] == "pending"]
    
    # Dodaj waiting_hours i urgency flag
    for review in pending:
        try:
            submitted = datetime.strptime(review["submitted_at"], "%Y-%m-%d %H:%M:%S")
            waiting_hours = (datetime.now() - submitted).total_seconds() / 3600
            review["waiting_hours"] = int(waiting_hours)
            review["is_urgent"] = waiting_hours >= GAME_MASTER_CONFIG["urgent_threshold_hours"]
        except Exception:
            review["waiting_hours"] = 0
            review["is_urgent"] = False
    
    # Sortuj: pilne na gÃ³rze, potem od najstarszych
    pending.sort(key=lambda x: (-x["is_urgent"], -x["waiting_hours"]))
    
    return pending


def get_reviewed_contracts(limit: int = 20) -> list:
    """Pobiera ostatnio ocenione kontrakty"""
    queue = load_game_master_queue()
    reviewed = [r for r in queue if r["status"] == "reviewed"]
    
    # Sortuj od najnowszych
    reviewed.sort(key=lambda x: x.get("reviewed_at", ""), reverse=True)
    
    return reviewed[:limit]


# =============================================================================
# STATYSTYKI
# =============================================================================

def get_evaluation_stats() -> dict:
    """Zwraca statystyki systemu oceny"""
    queue = load_game_master_queue()
    
    pending = [r for r in queue if r["status"] == "pending"]
    reviewed = [r for r in queue if r["status"] == "reviewed"]
    
    stats = {
        "total_reviews": len(queue),
        "pending": len(pending),
        "reviewed": len(reviewed),
        "avg_rating": sum(r["rating"] for r in reviewed) / len(reviewed) if reviewed else 0,
        "active_mode": get_active_evaluation_mode()
    }
    
    return stats


# =============================================================================
# EXPORT
# =============================================================================

__all__ = [
    'evaluate_contract_solution',
    'evaluate_heuristic',
    'evaluate_with_ai',
    'queue_for_game_master',
    'submit_game_master_review',
    'get_active_evaluation_mode',
    'set_active_evaluation_mode',
    'get_pending_reviews_count',
    'get_pending_contract_reviews',
    'get_reviewed_contracts',
    'get_evaluation_stats',
    'save_gemini_api_key',
    'load_gemini_api_key'
]
